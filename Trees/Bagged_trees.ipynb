{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages(library(ipred))\n",
    "suppressMessages(library(caret))\n",
    "suppressMessages(library(Metrics))\n",
    "suppressMessages(library(plyr))\n",
    "suppressMessages(library(e1071))\n",
    "\n",
    "# Ignoring warning for better presentation\n",
    "options(warn=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook covers a R-based approach to Bagged Trees. We'll start with implementing a simple bagged tree model and then evalaute the model using confusion matrix. Then we will look at a cross validated bagged model and compare its result with the previous model. Compared to a single decision tree model, bagged tree model promises to increase the accuracy of the resulting predictions and reduces variance by averaging a set of observations. However, unlike decision trees bagged trees are harder to understand, interpret and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>months_loan_duration</th><th scope=col>percent_of_income</th><th scope=col>years_at_residence</th><th scope=col>age</th><th scope=col>default</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 6 </td><td>4  </td><td>4  </td><td>67 </td><td>no </td></tr>\n",
       "\t<tr><td>48 </td><td>2  </td><td>2  </td><td>22 </td><td>yes</td></tr>\n",
       "\t<tr><td>12 </td><td>2  </td><td>3  </td><td>49 </td><td>no </td></tr>\n",
       "\t<tr><td>42 </td><td>2  </td><td>4  </td><td>45 </td><td>no </td></tr>\n",
       "\t<tr><td>24 </td><td>3  </td><td>4  </td><td>53 </td><td>yes</td></tr>\n",
       "\t<tr><td>36 </td><td>2  </td><td>4  </td><td>35 </td><td>no </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " months\\_loan\\_duration & percent\\_of\\_income & years\\_at\\_residence & age & default\\\\\n",
       "\\hline\n",
       "\t  6  & 4   & 4   & 67  & no \\\\\n",
       "\t 48  & 2   & 2   & 22  & yes\\\\\n",
       "\t 12  & 2   & 3   & 49  & no \\\\\n",
       "\t 42  & 2   & 4   & 45  & no \\\\\n",
       "\t 24  & 3   & 4   & 53  & yes\\\\\n",
       "\t 36  & 2   & 4   & 35  & no \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "months_loan_duration | percent_of_income | years_at_residence | age | default | \n",
       "|---|---|---|---|---|---|\n",
       "|  6  | 4   | 4   | 67  | no  | \n",
       "| 48  | 2   | 2   | 22  | yes | \n",
       "| 12  | 2   | 3   | 49  | no  | \n",
       "| 42  | 2   | 4   | 45  | no  | \n",
       "| 24  | 3   | 4   | 53  | yes | \n",
       "| 36  | 2   | 4   | 35  | no  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  months_loan_duration percent_of_income years_at_residence age default\n",
       "1  6                   4                 4                  67  no     \n",
       "2 48                   2                 2                  22  yes    \n",
       "3 12                   2                 3                  49  no     \n",
       "4 42                   2                 4                  45  no     \n",
       "5 24                   3                 4                  53  yes    \n",
       "6 36                   2                 4                  35  no     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reading data file ~ credit.csv \n",
    "# taken from https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29\n",
    "\n",
    "# credit.csv is composed of qualitative and quantitative variables.\n",
    "# For this exercise we will focus on the following variables only, namely -\n",
    "# months_loan_duration: colnumber - 2\n",
    "# percent_of_income: colnumber - 8\n",
    "# years_at_residence: colnumber - 9\n",
    "# age - 10\n",
    "# default - 17\n",
    "\n",
    "cols <- rep('NULL', 17)\n",
    "cols[c(2, 8, 9, 10, 17)] <- NA\n",
    "\n",
    "creditsub <- read.csv(file = '/home/dell/R_programming/case studies/trees/machinelearning-R/credit.csv', \n",
    "                      colClasses = cols,\n",
    "                      header = T)\n",
    "\n",
    "# Let's take a look at the dataframe\n",
    "head(creditsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have been following from my other notebook titled \"Decision Trees in R\" then you'll realize that we are using the same dataset. In the previous notebook we followed a decision tree model based approach and in this one we'll follow bagged trees approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the data into train and test\n",
    "\n",
    "# Setting seed for reproducible train and test partitions\n",
    "set.seed(123)\n",
    "\n",
    "smp_size <- floor(0.75 * nrow(creditsub))\n",
    "\n",
    "train_ind <- sample(seq_len(nrow(creditsub)), size = smp_size)\n",
    "\n",
    "credit_train <- creditsub[train_ind, ]\n",
    "\n",
    "credit_test <- creditsub[-train_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging classification trees with 25 bootstrap replications \n",
      "\n",
      "Call: bagging.data.frame(formula = default ~ ., data = credit_train, \n",
      "    coob = TRUE)\n",
      "\n",
      "Out-of-bag estimate of misclassification error:  0.344 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let us train our models.\n",
    "# Training a bagged model\n",
    "credit_model <- bagging(formula = default ~ ., \n",
    "                        data = credit_train,\n",
    "                        coob = TRUE)\n",
    "\n",
    "# Let's print the model\n",
    "print(credit_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the above cell, we used 'coob'=TRUE as one of the parameters to bagging. Assigning this true allows us to estimate the model's accuracy using the \"out-of-bag\" (OOB) samples. The OOB samples are the training obsevations that were not selected into the bootstrapped sample (used in training). Since these observations were not used in training, we can use them instead to evaluate the accuracy of the model (done automatically inside the bagging() function).\n",
    "\n",
    "#Let's make prediction with the model we just created and evaluate its performance with confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction  no yes\n",
       "       no  138  55\n",
       "       yes  37  20\n",
       "                                          \n",
       "               Accuracy : 0.632           \n",
       "                 95% CI : (0.5689, 0.6919)\n",
       "    No Information Rate : 0.7             \n",
       "    P-Value [Acc > NIR] : 0.99127         \n",
       "                                          \n",
       "                  Kappa : 0.0593          \n",
       " Mcnemar's Test P-Value : 0.07633         \n",
       "                                          \n",
       "            Sensitivity : 0.7886          \n",
       "            Specificity : 0.2667          \n",
       "         Pos Pred Value : 0.7150          \n",
       "         Neg Pred Value : 0.3509          \n",
       "             Prevalence : 0.7000          \n",
       "         Detection Rate : 0.5520          \n",
       "   Detection Prevalence : 0.7720          \n",
       "      Balanced Accuracy : 0.5276          \n",
       "                                          \n",
       "       'Positive' Class : no              \n",
       "                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Generate predicted classes using the model object# Gener \n",
    "class_prediction <- predict(object = credit_model,    \n",
    "                            newdata = credit_test,  \n",
    "                            type = \"class\")\n",
    "\n",
    "# Let's calculate the confusion matrix for the test set\n",
    "confusionMatrix(data = class_prediction,       \n",
    "                reference = credit_test$default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy of bagged model is even worse than the decision tree model, which gave accuracy around 70%.\n",
    "\n",
    "#Let's see if we can improve it somehow. If we look at the predict function above, we will realize that we have used type=\"class\" which provides a particular class as the predicted output. However, we could have passed type=\"prob\" which would have provided us with the probability of a data set belonging to that class. Let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>no</th><th scope=col>yes</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>0.88</td><td>0.12</td></tr>\n",
       "\t<tr><td>0.84</td><td>0.16</td></tr>\n",
       "\t<tr><td>0.36</td><td>0.64</td></tr>\n",
       "\t<tr><td>0.76</td><td>0.24</td></tr>\n",
       "\t<tr><td>1.00</td><td>0.00</td></tr>\n",
       "\t<tr><td>1.00</td><td>0.00</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       " no & yes\\\\\n",
       "\\hline\n",
       "\t 0.88 & 0.12\\\\\n",
       "\t 0.84 & 0.16\\\\\n",
       "\t 0.36 & 0.64\\\\\n",
       "\t 0.76 & 0.24\\\\\n",
       "\t 1.00 & 0.00\\\\\n",
       "\t 1.00 & 0.00\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "no | yes | \n",
       "|---|---|---|---|---|---|\n",
       "| 0.88 | 0.12 | \n",
       "| 0.84 | 0.16 | \n",
       "| 0.36 | 0.64 | \n",
       "| 0.76 | 0.24 | \n",
       "| 1.00 | 0.00 | \n",
       "| 1.00 | 0.00 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     no   yes \n",
       "[1,] 0.88 0.12\n",
       "[2,] 0.84 0.16\n",
       "[3,] 0.36 0.64\n",
       "[4,] 0.76 0.24\n",
       "[5,] 1.00 0.00\n",
       "[6,] 1.00 0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(object = credit_model,\n",
    "                newdata = credit_test,\n",
    "                type = \"prob\")\n",
    "                \n",
    "# Let's look at the pred format\n",
    "head(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we have the probabilities, we can decide the threshold value that will give the best result. The best way to choose a value is to compare the results at each threshold. If we measure area under the ROC curve, it will give us the error rate for one particular model. Let's check that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.532"
      ],
      "text/latex": [
       "0.532"
      ],
      "text/markdown": [
       "0.532"
      ],
      "text/plain": [
       "[1] 0.532"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "auc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n",
    "    predicted = pred[,\"yes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make sure that our result is consistent if we change the training and testing data set we should consider using cross-validation. let cross-validate a bagged tree model using caret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged CART \n",
      "\n",
      "750 samples\n",
      "  4 predictors\n",
      "  2 classes: 'no', 'yes' \n",
      "\n",
      "No pre-processing\n",
      "Resampling: Cross-Validated (5 fold) \n",
      "Summary of sample sizes: 600, 600, 600, 600, 600 \n",
      "Resampling results:\n",
      "\n",
      "  ROC        Sens       Spec     \n",
      "  0.5884868  0.8133333  0.3066667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the training configuration\n",
    "ctrl <- trainControl(method = \"cv\",     # Cross-validation\n",
    "                     number = 5,      # 5 folds\n",
    "                     classProbs = TRUE,                  # For AUC\n",
    "                     summaryFunction = twoClassSummary)  # For AUC\n",
    "\n",
    "\n",
    "credit_caret_model <- train(default ~ .,\n",
    "                            data = credit_train, \n",
    "                            method = \"treebag\",\n",
    "                            metric = \"ROC\",\n",
    "                            trControl = ctrl)\n",
    "\n",
    "print(credit_caret_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'method'</li>\n",
       "\t<li>'modelInfo'</li>\n",
       "\t<li>'modelType'</li>\n",
       "\t<li>'results'</li>\n",
       "\t<li>'pred'</li>\n",
       "\t<li>'bestTune'</li>\n",
       "\t<li>'call'</li>\n",
       "\t<li>'dots'</li>\n",
       "\t<li>'metric'</li>\n",
       "\t<li>'control'</li>\n",
       "\t<li>'finalModel'</li>\n",
       "\t<li>'preProcess'</li>\n",
       "\t<li>'trainingData'</li>\n",
       "\t<li>'resample'</li>\n",
       "\t<li>'resampledCM'</li>\n",
       "\t<li>'perfNames'</li>\n",
       "\t<li>'maximize'</li>\n",
       "\t<li>'yLimits'</li>\n",
       "\t<li>'times'</li>\n",
       "\t<li>'levels'</li>\n",
       "\t<li>'terms'</li>\n",
       "\t<li>'coefnames'</li>\n",
       "\t<li>'xlevels'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'method'\n",
       "\\item 'modelInfo'\n",
       "\\item 'modelType'\n",
       "\\item 'results'\n",
       "\\item 'pred'\n",
       "\\item 'bestTune'\n",
       "\\item 'call'\n",
       "\\item 'dots'\n",
       "\\item 'metric'\n",
       "\\item 'control'\n",
       "\\item 'finalModel'\n",
       "\\item 'preProcess'\n",
       "\\item 'trainingData'\n",
       "\\item 'resample'\n",
       "\\item 'resampledCM'\n",
       "\\item 'perfNames'\n",
       "\\item 'maximize'\n",
       "\\item 'yLimits'\n",
       "\\item 'times'\n",
       "\\item 'levels'\n",
       "\\item 'terms'\n",
       "\\item 'coefnames'\n",
       "\\item 'xlevels'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'method'\n",
       "2. 'modelInfo'\n",
       "3. 'modelType'\n",
       "4. 'results'\n",
       "5. 'pred'\n",
       "6. 'bestTune'\n",
       "7. 'call'\n",
       "8. 'dots'\n",
       "9. 'metric'\n",
       "10. 'control'\n",
       "11. 'finalModel'\n",
       "12. 'preProcess'\n",
       "13. 'trainingData'\n",
       "14. 'resample'\n",
       "15. 'resampledCM'\n",
       "16. 'perfNames'\n",
       "17. 'maximize'\n",
       "18. 'yLimits'\n",
       "19. 'times'\n",
       "20. 'levels'\n",
       "21. 'terms'\n",
       "22. 'coefnames'\n",
       "23. 'xlevels'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"method\"       \"modelInfo\"    \"modelType\"    \"results\"      \"pred\"        \n",
       " [6] \"bestTune\"     \"call\"         \"dots\"         \"metric\"       \"control\"     \n",
       "[11] \"finalModel\"   \"preProcess\"   \"trainingData\" \"resample\"     \"resampledCM\" \n",
       "[16] \"perfNames\"    \"maximize\"     \"yLimits\"      \"times\"        \"levels\"      \n",
       "[21] \"terms\"        \"coefnames\"    \"xlevels\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.588486772486773"
      ],
      "text/latex": [
       "0.588486772486773"
      ],
      "text/markdown": [
       "0.588486772486773"
      ],
      "text/plain": [
       "[1] 0.5884868"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the contents of the model list \n",
    "names(credit_caret_model)\n",
    "\n",
    "# Printing the CV AUC\n",
    "credit_caret_model$results[,\"ROC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So upon cross validation, we found that the performance of the model improved (at least on training set). Let's check the performance of the cross-validated bagged tree model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.494285714285714"
      ],
      "text/latex": [
       "0.494285714285714"
      ],
      "text/markdown": [
       "0.494285714285714"
      ],
      "text/plain": [
       "[1] 0.4942857"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred <- predict(object = credit_caret_model, \n",
    "                newdata = credit_test,\n",
    "                type = \"prob\")\n",
    "\n",
    "# auc\n",
    "auc(actual = ifelse(credit_test$default == \"yes\", 1, 0), \n",
    "                    predicted = pred[,\"yes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The performance of the cross-validated bagged tree model is 0.529 which could be taken as the expected auc of the model. This is almost no better than flipping a coin and telling if the person will default or not. We need to look at some other method now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.404"
      ],
      "text/latex": [
       "0.404"
      ],
      "text/markdown": [
       "0.404"
      ],
      "text/plain": [
       "[1] 0.404"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking a peek at CE as well\n",
    "pred <- predict(object = credit_caret_model, \n",
    "                newdata = credit_test,\n",
    "                type = \"raw\")\n",
    "\n",
    "# classification error (ce)\n",
    "ce(actual = credit_test$default, \n",
    "   predicted = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen classification error is 0.376 which is worse than that of decision trees, 0.288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
